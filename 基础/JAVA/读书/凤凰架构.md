### 单体系统时代

单体架构

”单体“只是表面系统中主要的过程调用都是进程内调用，不会发生进程间通信，仅此而已。

笔者认为单体应用被微服务应用取代的理由是：单体系统很难兼容Phoenix的特性。这种架构风格潜在的观念是希望系统的每一个部件，每一处代码都尽量可靠，靠不出货少出缺陷来构建可靠系统。【然而战术层面再优秀，也很难弥补战略层面的不足】，单体靠质量来保证高可靠性的思路，在小规模软件上还能运行良好，但系统规模越大，交付一个可靠的单体系统就变得越来越具有挑战性。构筑可靠系统从”追求尽量不出错“，到正规”出错是必然“的观念转变，才是微服务架构得以挑战并逐步开始取代运作了数十年的单体架构的底气所在。





### SOA时代

SOA架构

面向服务的架构是一次具体地、系统性地成功解决分布式服务主要问题的架构模式

三种较有代表性的架构模式：

* 烟囱性架构

  单独部署的系统，老死不相往来，两个不发生交互的信息系统

* 微内核架构

​		通过插件的方式来提供新功能的定制开发能力

* 事件驱动架构

SOA：应用受架构复杂度的牵绊越来越大，已经距离”透明“二字越来越远了





### 微服务时代

微服务是一种通过多个小型服务组合来构建单个应用的架构风格，这些服务围绕业务能力而非特定的技术标准来构建。各个服务可以采用不同的编程语言，不同的存储技术，运行在不同的进程之中。服务采取轻量级的通信机制和自动化的部署机制实现通信与运维。

* 围绕业务能力构建

* 分散治理

* 通过服务来实现独立自治的组件

* 产品化思维

* 数据去中心化

* 强终端弱管道

* 容错性设计

  不再虚幻地追求服务永远稳定，而是接受服务总是会出错的现实，要求在微服务的设计中，有自动的机制对其依赖的服务能够进行快速故障检测，在持续出错的时候进行隔离，在服务恢复的时候重新联通。

* 演进式设计

* 基础设施自动化





### 后微服务时代

从软件层面独立应对微服务架构问题，发展到软、硬一体，合力应对架构问题的时代



分布式架构的问题：这些问题一定要由软件系统自己来解决吗？

如果某个系统需要解决传输安全问题，通常会布置TLS传输链路，配置好CA证书来保证通信不被窃听篡改。如果需要解决服务发现问题，通常会设置DNS服务器，让服务访问依赖稳定的记录名而不是易变的IP地址



但基础设施是针对整个容器来管理的，粒度相对粗犷，只能到容器层面，对单个远程服务就很难有效管控。类似的情况不仅仅在断路器上出现，服务的监控、认证、授权、安全、负载均衡等都有可能面临细化管理的需求，譬如服务调用时的负载均衡，往往需要根据流量特征，调整负载均衡的层次、算法，等等，而DNS尽管能实现一定程度的负载均衡，但通常并不能满足这些额外的需求。



为了解决这一类问题，虚拟化的基础设施很快完成了第二次进化，引入了今天被称为”服务网格“(Service Mesh)的”边车代理模式“(Sidecar Proxy)



K8S将会称为服务器端标准的运行环境，如同现在的Linux系统；服务网格将会成为微服务之间通信交互的主流模式，把”选择什么通信协议“、”怎么调度流量“】”如何认证授权“之类的技术问题隔离于程序代码之外，取代今天Spring Cloud全家桶中的大部分组件的功能



业务与技术完全分离，远程与本地完全透明，也许这就是最好的时代了吧？





### 无服务时代

本来无服务也是以”简单“为主要卖点的，它只涉及两块内容：后端设施（Backend）和函数（Function）

* <strong>后端设施</strong>是指数据库、消息队列、日志、存储等等这一类用于支撑业务逻辑运行，但本身无业务含义的技术组件，这些后端设施都运行在云中，无服务中称其为”后端即服务“（Backend as a Service，BaaS）

* <strong>函数</strong>是指业务逻辑代码，这里函数的概念与粒度，都已经很接近于程序编码角度的函数了，其区别是无服务中函数运行在云端，不必考虑算力问题，不必考虑容量规划，无服务中称其为”函数即服务“（Function as a Service，FaaS）





### 远程调用服务

RPC

原来已经存在40多年了

进程间通信：

* 管道(Pipe)或者具名管道(Named Pipe)

​		管道类似于两个进程间的桥梁，可通过管道在进程间传递少量的字符流或字节流

​		举例：

```shell
ps -ef | grep java
```

ps与grep是两个独立的进程，以上命令就通过管道操作符 | 将ps命令的标准输出连接到grep命令的标准输入上

* 信号

​		信号用于通知目标进程有某种事发生，除了用于进程间的通信外，进程还可以发送信号给进程自身

​		举例：

```shell
kill -9 pid
```

* 信号量

​		信号量用于两个进程之间同步协作手段，它相当于操作系统提供一个特殊变量，程序可以在上面进程wait()和notify()操作

* 消息队列

​		可用于进程间数据量较多的通信

* 共享内存

​		允许多个进程访问同一块公共的内存空间，这是效率最高的进程间通信形式

* 套接字接口



webservice缺点：

​	1.webservice又是跨语言的RPC协议，这使得一个简单的字段，为了在不同语言中不会产生歧义，要以XML严谨描述的话，往往需要比原本存储这个字段多出十几倍、几十倍乃至上百倍的空间。这一特点一方面导致了使用webservice必须要专门的客户端去调用和解析SOAP内容，也需要专门的服务去部署，更关键是导致了每一次数据交互都包含了大量的冗余信息，性能奇差。

​	2.webservice还有另外一点原罪：贪婪



​	没有完美的RPC协议，不能同时满足简单、舒适、高性能。



### REST设计风格

REST和RPC在思想上差异的核心是抽象的目标不一样，即面向资源的编程思想与面向过程的编程思想两者之间的区别

REST：表征状态转移

RESTful的系统：

* 服务端与客户端分离(Client -Server)
* 无状态(Stateless)
* 可缓存
* 分层系统
* 统一接口



不足与争议：

面向资源的编程思想只适合做CRUD，面向过程、面向对象编程才能处理真正复杂的逻辑



### 事务

事务的概念起源于数据库系统，但是现在已经不局限于数据库本身，所有包括数据一致性的应用场景包括数据库、事务内存、缓存、消息队列、分布式存储等

研究事务的实现原理，必定会追溯到ARIES理论。

通过日志实现事务的原子性和持久性是当今的主流方案，

被称为Commit Log方式：

* 提交记录（Commit Record）
* 追加结束日志（End Record）

Shadow Paging：SQLITE3 正在使用。"修改指针"这个操作被认为是"原子操作"，现代磁盘的写操作可以认为在硬件上保证了不会出现"半个值"的现象

* 一份修改前的数据、一份修改后的数据
* 修改数据的引用指针，将引用从原数据改为新复制出来修改后的副本



AIRES提出"Write-Ahead Logging"的日志改进方案，所谓提前写入

它在崩溃恢复时会执行以下三个阶段：

* 分析阶段
* 重做阶段
* 回滚阶段



STEAL 内容说明：

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211221113205975.png" alt="image-20211221113205975" style="zoom:100%;margin-left:5em" />



<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211221112213691.png" alt="image-20211221112213691" style="zoom:50%;margin-left:-1px" />



##### 实现隔离性

现代数据库提供以下三个锁：

* 写锁

​		数据加持着写锁时，其他事务不能写入数据，也不能施加读锁

* 读锁

​		对于持有读锁的事务，如果该数据只有它自己一个事务加了读锁，允许直接将其升级为写锁，然后写入数据

* 范围锁

  对于某个范围直接加了排他锁，在这个范围内数据不能被写入

现代数据库一定会提供除”可串行化“以外的其他隔离级别供用户使用，让用户自行选择调节隔离级别，取隔离性与吞吐量之前的平衡。





##### 本地事务

* 串行化 Serializable
* 可重复读 Repeatable Read（MYSQL/INNODB的默认隔离级别）

​		比串行化弱化的地方在于幻读问题（Phantom Reads）。可重复读没有范围锁来禁止在该范围内插入新的数据

* 读已提交 Read Committed

​		比可重复读弱化的地方在于不可重复读问题

​		隔离级别缺乏贯穿整个事务周期的读锁，无法禁止读取过的数据发生变化

* 读未提交 Read Uncommitted

  比读已提交弱化的地方在于脏读问题

​		对事务涉及到的数据只加写锁，会一直持续到事务结束，但完全不加读锁



​	MVCC 多版本并发控制

​	MVCC是一种读取优化策略，它的”无锁“是指读取时不需要加锁。MVCC的思路是对数据库的任何修改都不会直接覆盖之前的数据，而是产生一个新版副本与老版本共存，以此达到读取时可以完全不加锁的目的。

​	你不妨将版本理解为数据库中每一行记录都存在两个看不见的字段：CREATE_VERSION和DELETE_VERSION，这两个字段记录的值都是事务ID，事务ID是一个全局严格递增的数值。

* 隔离级别是可重复读

​		总是读取CREATE_VERSION小于或等于当前事务的ID的记录，在这个前提下，如果数据仍存在多个版本，则取最新（事务ID最大）的

* 隔离级别是读已提交

​		总是取最新的版本即可，即最近被Commit的那个版本的数据记录

​	



##### 全局事务（外部事务）

​		单服务多数据源的场合，有一套名为X/OPEN XA的处理事务的架构，其核心内容是定义了全局的事务管理器（Transaction Manager，用于协调全局事务）和局部的资源管理器（Resource Manager，用于驱动本地事务）之间的通信接口

​		基于XA模式在Java语言中的实现全局事务处理的标准，这也是我们现在熟知的JTA

​		为了解决一部分数据被提交，另一部分被回滚，整个事务的一致性保证问题，XA将事务提交拆分成为**两阶段**过程：

* 准备阶段
* 提交阶段

​		以上两个过程被称为”两阶段提交（2 Phase Commit，2PC ）“ 协议

​		协调者、参与者都是由数据库自己来扮演的，而应用程序相对于数据库来说只扮演客户端的角色

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211222160628680.png" alt="image-20211222160628680" style="zoom:50%" />



​	三阶段提交

* CanCommit

​		增加了一轮询问阶段，如果都得到了正面的响应，那事务能够成功提交的把握性就比较大了。

* PreCommit
* DoCommit

​		<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211222162500301.png" alt="image-20211222162500301" style="zoom:50%;" />



##### 共享事务

多个服务共用同一个数据源

一个服务集群里数据库才是压力最大又不容易伸缩拓展的重灾区，所以现实中只有类似ProxySQL、MaxScale这样用于对多个数据库实例做负载均衡的数据库代理



##### 分布式事务

书中所指的分布式事务特指多个服务同时访问多个数据源的事务处理机制

CAP：一个分布式系统中，涉及共享数据问题时，以下三个特性最多只能满足其中两个

* 一致性（Consistency）
* 可用性（Availability）
* 分区容忍性（Partition Tolerance）



放弃可用性：在现实中，选择放弃可用性的CP系统一般用户对数据质量要求很高的场合中

所以后面就有了强一致性（线性一致性）、弱一致性（最终一致性）

​							刚性事务、柔性事务



##### 柔性事务几种常见做法

* 可靠事件队列

​		使用BASE来达成一致性目的的途径，BASE分为基本可用性（Base Available）、柔性事务（Soft State）和最终一致性（Eventually Consistent）

​		<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211222180120752.png" alt="image-20211222180120752" style="zoom:50%;" />

​		不断重试，在账户服务数据库建立一张消息表，里面存入消息，事务ID：UUID，出库（说明，状态），收款说明（状态，数量）

​		需满足幂等性

* TCC事务

​		另一种常见的分布式事务机制

​		Try - Confirm - Cancel

​		上面可靠事件队列缺乏隔离性，有超售的可能性。如果业务需要隔离，那架构师通常就应该重点考虑TCC方案了，该方案天生适用需要强隔离性的分布式事务中

​		<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211223110508820.png" alt="image-20211223110508820" style="zoom:50%;" />

​	更高的开发成本和更换事务实现方案的替换成本，通常我们并不会完全靠裸代码来实现TCC，而是基于某些分布式事务中间件（譬如阿里开源的Seata）去完成，尽量减轻一些编码工作量

* SAGA事务

​		TCC其性能是比较高的柔性事务，但它仍不能满足所有的场景，它所要求的技术可控性上的约束比较大

​		将一个分布式环境中的大事务分解成一系列本地事务的设计模式

​		SAGA必须保证所有子事务都得以提交或者补偿

​		SEATA同样支持SAGA事务模式

在读隔离方面，AT事务默认的隔离级别是读未提交（Read Uncommitted）





### 透明多级分流系统

##### 域名解析

专门有一种称为<strong style="color:blue">DNS领取(DNS Prefetching)</strong>

##### 传输链路

连接数优化：TCP有慢启动的特性，使得刚刚建立连接时传输速度是最低的，后面再逐步加速至稳定。

TCP是可靠传输的，所以一个错误的TCP包会导致所有的流都必须等待这个包重传成功，HTTP2采用多路复用的技术

HTTP3 可能是基于UDP做设计的，QUIC

##### 内容分发网络

* 路由解析
* 内容分发
* 负载均衡

两张图，对比有无内容分发网络参与的情况：

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211223182234847.png" alt="image-20211223182234847" style="zoom:50%;margin-left:-1px" />

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211223182255827.png" alt="image-20211223182255827" style="zoom:50%;margin-left:-1px" />



##### 负载均衡

无论在网关内部建立了多少级负载均衡，从形式上来说都可以分为两种：

* 四层负载均衡
* 七层负载均衡





##### 服务器缓存

并发读写场景中，吞吐量受多方面的共同影响，譬如，怎么设计数据结构以尽可能避免数据竞争？

存在竞争风险时怎么处理同步（主要是使用锁实现悲观同步和使用CAS实现的乐观同步）

缓存淘汰策略：

* FIFO（First in Fisrt Out）优先淘汰最早进入被缓存的数据
* LRU（Least Recent Used）：优先淘汰最久未被使用访问过的数据

​		HashMap加LinkedList双重结构（如LinkedHashMap）来实现

* LFU（Least Frequentlu Used）：优先淘汰最不经常使用的数据

​		LFU会给每个数据添加一个访问计数器，每访问一次就加1，需要淘汰时就清理计数器最小的那批数据。

* TinyLFU
* W-TinyLFU



<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211224174858870.png" alt="image-20211224174858870" style="zoom:50%;margin-left:-1px" />



分布式缓存：

* 复制式缓存：适用于极少更新但读取频繁的数据
* 集中式缓存：对于更新和读取较为频繁的数据，代表如Redis

​	

  缓存风险：

* 缓存穿透（恶意攻击）

​		1.业务逻辑梳理整理

​		2.加一层布隆过滤器

* 缓存击穿（缓存中某些热点数据忽然因某种原因失效了）

​		1.加锁同步

​		2.热点数据由代码手动管理

* 缓存雪崩（大批不同的数据在短时间内一起失效）

  1.提升缓存系统可用性，建设分布式缓存的集群

  2.启动透明多级缓存

  3.将缓存的生存期从固定时间改为一个时间内的随机时间

* 缓存污染（开发不规范导致的）

​		成本最低的Cache-Aside模式是指：

​		1.读数据时，先读缓存，缓存没有的话，再读数据源，然后将数据放入缓存，返回响应请求

​		<strong style="color:blue">2.写数据时，先写数据源，然后失效（而不是更新）掉缓存</strong>

​		还有其他更新缓存的模式，如Read/Write Through、Write Behind Caching。Cache-Aside 模式依然是不能保证在一致性上绝对不出问题的



### 架构安全性

##### 认证：系统如何正确分辨出用户的真实身份

Http认证

响应报文头header中附带的信息

```http
WWW-Authenticate:<认证方案> realm=<保护区域的描述信息>
Proxy-Authenticate:<认证方案> realm=<保护区域的描述信息>
```



客户端请求头包含以下header信息

```http
Authorization:<认证方案><凭证内容>
Proxy-Authorization:<认证方案><凭证内容>
```

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211227104004199.png" alt="image-20211227104004199" style="zoom:50%;margin-left:-1px" />

Http Basic认证是主要以演示为目的的认证方案，也应用于一些不要求安全性的场合，譬如家里的路由器登录等





Web认证

依靠内容而不是传输协议来实现的认证方式。由于形式上登录表单占了绝对的主流，因此通常也被称为“表单认证”

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211227134827057.png" alt="image-20211227134827057" style="zoom:50%;margin-left:-1px" />



##### 授权

* 确保授权的过程可靠

  OAuth2和SAML2.0

* 确保授权的结果可控

​		授权的结果用于对程序功能或者资源的访问控制，成理论体系的权限控制模型有很多，譬如自主访问控制（Discretionary Access Control  DAC）、强制访问控制（Mandatory Access Control  MAC）、基于属性的访问控制（Attribute-Based Access Control  ABAC）、还有最常用的基于角色的访问控制（Roled-Based Access Control RBAC）



RBAC

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211227143600330.png" alt="image-20211227143600330" style="zoom:50%;margin-left:-1px" />

RBAC还允许对不同角色之间的关联与约束，进一步强化它的抽象描述能力

RBAC-1：角色权限的继承关系

RBAC-2：角色职责分离关系



Spring Security 的访问控制模型：

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211227153100269.png" alt="image-20211227153100269" style="zoom:50%;margin-left:-1px" />



##### OAuth2

以令牌（Token）代替用户密码作为授权的凭证。

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211227154205230.png" alt="image-20211227154205230" style="zoom:50%;margin-left:-1px" />

OAuth2 一共提供了四种不同的授权方式

* 授权码模式

​		<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211227155044080.png" alt="image-20211227155044080" style="zoom:50%;margin-left:-1px" />

* 隐式授权模式

​		<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211227160317242.png" alt="image-20211227160317242" style="zoom:50%;margin-left:-1px" />

* 密码模式

​		<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211228105055345.png" alt="image-20211228105055345" style="zoom:50%;margin-left:-1px" />

* 客户端模式

​		<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211228105345809.png" alt="image-20211228105345809" style="zoom:50%;margin-left:-1px" />







##### 凭证

##### Cookie-Session

基于http是无状态的协议，每次资源请求互不影响

RFC-6265在HTTP协议中增加了Set-Cookie指令

Cookie-Session方案在“安全性”上具有一定先天优势：状态信息都存储于服务器

另一大优点是服务端有主动的状态管理能力，可根据自己的意愿随时修改，清除任意上下文信息，譬如很轻易就能实现强制某用户下线

Session-Cookie在单节点的单服务环境是最合适的方案，在分布式环境会遇到CAP不可兼得的问题



##### JWT

它最常见的使用方式是附在名为Authorization的Header发送给服务器端

JWT只解决防篡改的问题，并不解决防泄漏的问题

* 令牌头

​		<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211228151816323.png" alt="image-20211228151816323" style="zoom:50%;" />

* 负载
* 签名



看来客户端加密传输 ?

慢哈希函数是指这个函数执行时间是可以调节的哈希函数

验证过程与加密：

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211228190448756.png" alt="image-20211228190448756" style="zoom:50%;margin-left:-1px" />





传输安全层

TLS连接握手

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211229104900961.png" alt="image-20211229104900961" style="zoom:50%;margin-left:-1px" />



##### Paxos(基于操作转移模型来设计的算法)

* 提案节点

​		称为Proposer，提出对某个值进行设置操作的节点，设置值这个行为被称之为提案

* 决策节点

​		称为Acceptor，是应答提案的节点，决定该提案是否可被投票、是否可被接受。提案一旦得到过半数决策节点的接受，即称该提案被批准

* 记录节点

​		被称为Learner，不参与提案，也不参与决策



整体时序图：

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211229152853021.png" alt="image-20211229152853021" style="zoom:50%;margin-left:-1px" />



Basic Paxos的价值在于开拓了分布式共识算法的发展思路，但一般不会用于实践。现在几乎只用来做理论研究。实际的应用都是基于Multi Paxos和Fast Paxos算法来的。接下来会介绍Multi Paxos 与它一些理论等价的算法（Raft、ZAB等）



<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211229162742775.png" alt="image-20211229162742775" style="zoom:50%;margin-left:-1px" />





<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211229162921489.png" alt="image-20211229162921489" style="zoom:50%;margin-left:-1px" />



以下三个问题同时被解决时，即等价于达成共识：

* 如何选主（Leader Election）
* 如何把数据复制到各个节点上（Entity Replication）
* 如何保证过程是安全的（Safety）





Gossip协议：

* 如果有某一项信息需要在整个网络中所有节点中传播，那从信息源开始，选择一个固定的周期（譬如1秒），随机选择它相连接的k个节点（称为Fan-Out）来传播消息
* 每一个节点收到消息后，如果这个消息是它之前没有收到过的，选择除了发送消息给它的那个节点外的其他相邻K个节点发送相同的消息，直到最终网络中所有节点都收到了消息，尽管这个过程需要一定时间，但是理论上最终网络的所有节点都会拥有相同的信息。

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211229170907009.png" alt="image-20211229170907009" style="zoom:50%;margin-left:-1px" />







##### 服务发现

总的来说经历三个过程：

* 服务的注册
* 服务的维护
* 服务的发现







<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20211231112615938.png" alt="image-20211231112615938" style="zoom:50%;margin-left:-1px" />



当下直接以服务发现、服务注册中心为目标的组件库，或者间接用来实现这个目标的工具主要有如下三类：

* 在分布式K/V存储框架上自己开发的服务发现，代表是ZK、Doozerd、ETCD
* 以基础设施（主要是DNS服务器）来实现服务发现，这类代表是SkyDNS、CoreDNS
* 专注于服务发现的框架和工具，代表是Eureka、Consul、Nacos



##### 网关

微服务网关的首要职责就是作为统一的出门对外提供服务，将外部访问网关地址的流量，根据适当的规则路由到内部集群中正确的服务器节点之上。

因为今天REST和JSON-RPC等基于HTTP协议的服务接口在对外部提供的服务中占绝对主流的地位，所以我们所讨论的服务网关默认都必须支持七层路由。 



代理负载均衡器：

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220101115656964.png" alt="image-20220101115656964" style="zoom:50%;margin-left:-1px" />

k8s严格保证了同一个pod中的容器不会跨越不同的节点，这些容器共享着同一个网络名称空间，因为代理均衡器与服务实例的交互，实质上是对本机回环设备的访问，仍然要比真正的网络交互高效且稳定得多。代理均衡器付出的代价较小，但从服务进程中分离出来所获得的收益却是非常显著的。

边车代理



地域与区域

* Region：地域的意思，譬如华北、东北、华东、华南，这些都是地域范围。不同区域之间是没有内网连接的，所有流量只能经过公众互联网相连。
* Zone：区域指在地理上位于同一地域内，但电力和网络是互相独立的物理区域。同一个地域的可用区域之间具有内网连接，流量不占用公网带宽。

​		容灾是非实时的同步，而双活是实时或者准实时的





##### 服务容错

容错策略：

* 故障转移

​		有重试成本，如重试三次

* 快速失败

​		非幂等服务，重复调用就可能产生脏数据。在支付场景中，需要调用银行的扣款接口，如果该接口返回的结果是网络异常，程序是很难判断到底是扣款指令发送给银行时出现的网络异常，还是银行扣款后返回结果给服务时出现的网络异常的。

| 容错策略 | 优点                                               | 缺点                                                         | 应用场景                                               |
| :------- | -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------ |
| 故障转移 | 系统自动处理，调用者对失败的信息不可见             | 增加调用时间，额外的资源开销                                 | 调用幂等服务对调用时间不敏感的场景                     |
| 快速失败 | 调用者有对失败的处理完全控制权，不依赖服务的幂等性 | 调用者必须正确处理失败逻辑，如果一味只是对外抛异常，容易引起雪崩 | 调用非幂等性服务超时阈值较低的场景                     |
| 安全失败 | 不影响主路逻辑                                     | 只适用于旁路调用                                             | 调用链中的旁路服务                                     |
| 沉默失败 | 控制错误不影响全局                                 | 出错的地方将在一段时间内不可用                               | 频繁超时的服务                                         |
| 故障恢复 | 调用失败后自动重试，也不影响主路逻辑               | 重试任务可能产生堆积，重试任然可能失败                       | 调用链中的旁路服务对实时性要求不高的主路逻辑也可以使用 |
| 并行调用 | 尽可能在最短时间内获得最高的成功率                 | 额外消耗机器资源，大部分调用可能都是无用功                   | 资源充足且对失败容忍度低的场景                         |
| 广播调用 | 支持同时对批量的服务提供发起调用                   | 资源消耗大，失败概率高                                       | 只适用于批量操作的场景                                 |







##### 容错设计模式

断路器模式

基本思路很简单，就是通过代理（断路器对象）来一对一地（一个远程服务对应一个断路器）地接管服务调用者的远程请求。当出现故障（失败、超时、拒绝）的次数达到断路器的阈值时，它状态就自动变为“OPEN”

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220104104631499.png" alt="image-20220104104631499" style="zoom:50%;margin-left:-1px" />

从调用序列来看，断路器就是一种有限状态机，断路器模式就是根据自身状态变化自动调整代理请求策略的过程

* CLOSED
* OPEN
* HALF-OPEN

​		这是一种中间状态。断路器必须带有自动的故障恢复能力，当进入OPEN状态一段时间后，将“自动”切换到HALF-OPEN状态，该状态下，会放行一次远程调用，然后根据这次调用结果成功与否，转换为CLOSED或者OPEN状态，以实现断路器的弹性修复



舱壁隔离模式

为了不让某一个远程服务的局部失败演变成全局性影响，就必须设置某种止损方案，这便是服务隔离的意义

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220104135158339.png" alt="image-20220104135158339" style="zoom:50%;margin-left:-1px" />



重试模式

是否应该且是否能够对一个服务进行重试时，应同时满足几个前提条件：

* 仅在具备幂等性的服务进行重试，譬如RESTFul中的POST请求是非幂等的，而GET、HEAD、OPTIONS、TRACE由于不会改变资源状态，这些请求应该被设计成幂等的；PUT请求一般是幂等的，DELETE操也可以看作是幂等的





流量统计指标

* 每秒事务数（Transactions per Seconds，TPS）
* 每秒请求数（Hits per Seconds，HPS）
* 每秒查询数（Query per Seconds，QPS）



限流的设计模式

* 流量计数器

​		流量计数器的缺陷在于它只是针对时间点进行离散的统计

* 滑动时间窗

​		在计算机领域都有成功的应用，譬如编译原理中的窥孔优化（Peephole Optimization）、TCP协议的拥塞控制。

​		对于分布式系统来说，无论是服务容错中对服务响应结果的统计，还是流量控制中对服务请求数量的统计，都经常用到滑动窗口算法

​		它通常只适用于否决式限流，超过阈值的流量就必须强制失败或者降级，很难进行阻塞等待处理，也就很难在细粒度上对流量曲线进行整形，起不到削峰填谷的作用

* 漏桶

  水池作例子

  在代码上实现非常简单，它其实就是一个以请求对象作为元素的先入先出队列（FIFO QUEUE），队列长度就相当于漏桶的大小。困难在于如何确定漏桶的两个参数：桶的大小和水的流出速率

* 令牌桶

​		如果说漏桶是小学应用题中的奇怪水池，那令牌桶就是你去银行办事时摆在门口的那台排队机。它与漏桶一样都是基于缓冲区的限流算法，只是方向刚好相反，漏桶是从水池里往系统出水，令牌桶是系统往排队机中放入令牌





分布式限流

我们把前面介绍的限流模式都统称为单机限流，把能够精细控制分布式集群中每个服务消耗量的限流算法称为分布式限流



##### Sidecar[英文释义为摩托车上的跨斗]

将本将属于应用程序的功能拆分成单独的进程，这个进程可以被理解为Sidecar。在微服务体系内，将集成在应用内的微服务功能剥离到了Sidecar内，Sidecar提供了微服务发现、注册、服务调用、应用认证、限速等功能

特点：

* 为独立部署的进程
* 降低应用程序代码和底层代码的耦合度，帮助异构服务通过Sidecar快速接入微服务体系





事件日志

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220104182431198.png" alt="image-20220104182431198" style="zoom:50%;margin-left:-1px" />

建议输出到日志中的内容

* 处理请求的TraceID。服务收到请求没有附带TraceID，就应该自动生成唯一的TraceID进行标记，并使用MDC自动输出到日志，TraceID会贯穿整条调用链，目的是通过它把请求在分布式系统各个服务中执行过程串联起来
* 系统运行过程中的关键事件
* 启动时输出配置信息



##### 收集与缓冲

在Logstas之前架设一个Kafka或者Redis作为缓冲层，面对突发流量，Logstash或者ElasticSearch处理能力出现瓶颈时自动削峰填谷，甚至当它们短时间停顿时，也不会丢失日志数据



##### 链路追踪

追踪与跨度

从客户端发起请求抵达系统的边界开始，记录请求流经每一个服务，直到到客户端返回响应为止，这整个过程就称为一次“追踪”（Trace）。由于每次Trace都可能会调用数量不定，坐标不定的多个服务，为了能够记录具体调用了哪些服务，以及调用顺序、开始时点、执行时长等信息，每次开始调用服务都要先埋入一个调用记录，这个记录称为一个“跨度”（Span）

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220105145038425.png" alt="image-20220105145038425" style="zoom:50%;margin-left:-1px" />





##### 数据收集

* 基于日志的追踪（Log-Based Tracing）

  Spring Cloud Sleuth

* 基于服务的追踪（Service-Based Tracing）

​		Zipkin、SkyWalking、Pinpoint

​		Pinpoint本身就是比较重负载的系统（运行它必须先维护一套HBase），这严重制约了它的适用范围

​		目前服务追踪的一个发展趋势是轻量化，国产的Skywalking正是这方面的佼佼者

* 基于边车代理的追踪（SideCar-Based Tracing）

  服务网格的专属方案也是最理想的分布式追踪模型

  优点是对应用完全透明，与程序语言无关，有独立的数据通道，追踪数据通过控制平面进行上报，避免了追踪对程序通信或者日志归集的依赖和干扰，保证了最佳的精确性。

  缺点是只能实现服务调用层面的追踪，像Pinpoint本地方法调用级别的追踪诊断是做不到的

  Envoy 没有提供自己的界面端和存储端，现在Skywalking、Zipkin、Jaeper、LightStep Tracing等系统都可以接受来自Enovy的追踪数据，充当它的界面端





##### 追踪规范化

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220105154627483.png" alt="image-20220105154627483" style="zoom:50%;margin-left:-1px" />



##### 聚合度量

Prometheus

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220105155309344.png" alt="image-20220105155309344" style="zoom:50%;margin-left:-1px" />



每一个度量指标由时间戳、名称、值和一组标签构成，除了时间之外，指标不与任何其他因素相关。指标的数据总量固然是不小的，但它没有嵌套、没有关联、没有主外键、不必关心范式和事务 即 时序数据库

* 以日志结构的合并树（Long structured Merge Tree，LSM-Tree）代替传统关系型数据库中的B+Tree作为存储结构，LSM适合场景就是写多读少，且几乎不删改的数据
* 设置激进的数据保留策略：譬如根据过期时间（TTL）自动删除相关数据
* 对数据进行再采集（Resampling）以节省空间：譬如最近几天的数据需要精确到秒，而查询一个月前的时，只需精确到天 ......





##### 容器

* 隔离访问：namespace

* 隔离资源：cgroups

​		资源配额包括处理器时间、内存大小、磁盘I/O速度等等

* 封装系统：LXC

* 封装应用：Docker

​		<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220105182019063.png" alt="image-20220105182019063" style="zoom:50%;margin-left:-1px" />

* 封装集群：k8s

​		k8s容器编排框架 把大型软件系统运行所依赖的集群环境也进行了虚拟化，令集群得以实现跨数据中心的绿色部署，并能够根据实际情况自动扩缩

​		<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220105182424141.png" alt="image-20220105182424141" style="zoom:50%;margin-left:-1px" />



​	从k8s1.10版本宣布支持container1.1，在调用链中已经能够完全抹去Docker Engine 的存在：

```
Kubernetes Master → kubelet → KubeGenericRuntimeManager → containerd → runC
```





### 以容器构建系统

分布式系统里应用的概念已不再等同于进程，此时的应用需要多个进程共同协作，通过集群的形式对外提供服务，以虚拟化方法实现这个目标的过程就被称为容器编排



容器组的概念：多个容器放到同一个Pod中即可解决。扮演容器组的角色，满足容器共享名称空间的需求。它们将默认共享：

UTS名称空间：所有容器都有相同的主机名和域名

网络名称空间：所有容器都共享一样的网卡、网络栈、IP地址等等，因此同一个Pod中不同容器占用的端口不能冲突

IPC名称空间：所有容器都可以通过信号量或者POSIX共享内存等方式通信

时间名称空间：所有容器都共享相同的系统时间



k8s中一切皆以资源

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220108115523491.png" alt="image-20220108115523491" style="zoom:50%;margin-left:-1px" />



* 容器（Container）：延续了自Docker以来一个容器封装一个应用进程的理念，是镜像管理的最小单位
* 生产任务（Pod）：补充了容器化后缺失与进程组对应的”容器组“的概念，Pod中容器共享UTS、IPC、网络等名称空间，是资源调度的最小单位
* 节点（Node）：对应于集群中的单台机器，这里的机器即可以是生产环境中物理机，也可以是云计算环境中的虚拟节点，节点是处理器和内存等资源的资源池，是硬件单元的最小单位
* 集群（Cluster）：对应于整个集群，k8s提倡理念是面向集群来管理应用。当你要部署应用的时候，只需要通过声明式API将你的意图写成一份元数据（Manifest），将它提交给集群即可，而需关心它具体分配到哪个节点（尽管通过标签选择器完全可以控制它分配到阿哥节点，但一般不需要这样做）、如何实现Pod间通信、如何保证韧性与弹性，等等，所以集群是处理元数据的最小单位
* 集群联邦（Federation）：对应于多个集群，通过联邦可以统一管理多个k8s集群，联邦的一种常见应用是支持跨可用区域多活、跨地域容灾的需求



控制器模式是继资源模型之后，另一个k8s核心理念

让编排系统在这些服务出现问题，运行状态不正确的时候，能自动将它们调整成正确的状态，叫做控制回路

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220108230212076.png" alt="image-20220108230212076" style="zoom:50%;margin-left:-1px" />

学习k8s更好的方式是站在解决问题的角度去理解为什么k8s要设计这些资源和控制器，理解为什么这些资源和控制器会被设计成现在这种样子



Kustomize工具：

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220109173154979.png" alt="image-20220109173154979" style="zoom:50%;margin-left:-1px" />



Helm与Chart

更具系统性的管理和封装应用的解决方案

如果说K8s是云原生操作系统的话，那Helm就要成为这个操作系统上面的应用商店与包管理工具

缺点：Helm无法很好地管理这种有状态的依赖关系



Operator与CRD

把应用封装为另一种更高层次的资源，再把k8s的控制器模式从面向于内置资源，扩展到了面向所有自定义资源，以此来完成对复杂应用的管理



无状态应用天生就是高可用的。但不幸的是现在的分布式系统中多数关键的基础服务都是有状态的，如缓存、数据库、对象存储、消息队列，等等，只有Web服务器这类服务属于无状态的

针对有状态的应用，k8s发布了StatefulSet及对应的StatefulSetController，由StatefulSet管理的Pod具备以下的几项额外的特性：

* Pod会按顺序创建和按顺序销毁
* Pod具有稳定的网络名称
* Pod具有稳定的持久存储



Linux网络虚拟化

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220109210227496.png" alt="image-20220109210227496" style="zoom:50%;margin-left:-1px" />

程序发送数据做的是层层封包，加入协议头，传给下一层；接受数据则是层层解包，提取协议体，传给上一层







### 干预网络通信

这套名为Netfilter的框架是Linux防火墙和网络的主要维护者Rusty Russell提出并主导设计的，它围绕网络层（IP协议）周围，埋下了五个钩子（Hooks）

（1）PREROUTING

（2）INPUT

（3）FORWARD

（4）OUTPUT

（5）POSTROUTING

<img src="/Users/yangli/Library/Application Support/typora-user-images/image-20220109220010154.png" alt="image-20220109220010154" style="zoom:50%;margin-left:-1px" />



iptables内置了五张不可扩展的规则表

1.raw表：用于去除数据包上的连接追踪机制（Connection Tracking）

2.mangle表：用于修改数据包的报文头信息，如服务类型（Type of Service，ToS）、生存周期（Time to Live，TTL）以及为数据设置Mark标记，典型的应用是链路的服务质量管理（Quality of Service，Qos）

3.nat表：用于修改数据包的源或者目的地址等信息，典型的应用是网络地址转换（Network Address Translation）

4.filter表：用于对数据包进行过滤，控制到达某条链上的数据包是继续放行、直接丢弃或拒绝（ACCEPT、DROP、REJECT），典型的应用时防火墙

5.security表：用于在数据包上应用SELinux，这张表并不常见



以上五张规则表是具有优先级的：raw->mangle->nat->filter->security

iptables不仅仅是Linux系统自带的一个网络工具，它在容器间通信中扮演相当重要的角色，譬如k8s用来管理Service的Endpoints的核心组件kube-proxy，就依赖iptables来完成ClusterIP到Pod的通信（也可以采用IPVS、IPVS同样是基于Netfilter的），这种通信的本质就是一种NAT访问